#ifndef SELENE_UVM
#define SELENE_UVM

#include <selene/cpu.txt>
#include <selene/pmalloc.txt>
#include <selene/std.txt>
#include <selene/panic.txt>
#include <selene/interrupt.txt>
#include <selene/oom.txt>

#define USEG 0x8000000

#define PF_PRESENT 1
#define PF_DIRTY 2
#define PF_NR 4
#define PF_NW 8
#define PF_NX 16

struct pgdir {
    char ents[PAGESIZE];
}

#define MODE_NONE 0
//#define MODE_MAP MODE_NONE
#define MODE_MAP 0
#define MODE_READ 1
#define MODE_WRITE 2
#define MODE_EXECUTE 3
#define MODE_ACCESS_MASK 3
#define MODE_POPULATE 4

#define UVM_NEXIST 0
#define UVM_PROTECTION 1
//#define UVM_NUM_ERROR PAGESIZE
#define UVM_NUM_ERROR 128

char *uvm_resolve(pgdir *pt, char ent, char mode) {
    char pte;
    char memf = insn_cpuget(REG_MF);
    ent = ent & PAGEMASK;
    if(memf) CLM;
    pte = pt[ent];
    if(memf) STM;
    if(!(pte & PF_PRESENT)) {
        if(mode & MODE_POPULATE) {
            pte = pmalloc();
            while(!pte) {
                out_of_pmemory();
                pte = pmalloc();
            }
            pte = bits_remove(pte, PAGEMASK) | PF_PRESENT;
            if(memf) CLM;
            pt[ent] = pte;
            if(memf) STM;
        } else {
            return UVM_NEXIST;
        }
    }
    if((mode & MODE_ACCESS_MASK) == MODE_READ) {
        if(pte & PF_NR) return UVM_PROTECTION;
    }
    if((mode & MODE_ACCESS_MASK) == MODE_WRITE) {
        if(pte & PF_NW) return UVM_PROTECTION;
    }
    if((mode & MODE_ACCESS_MASK) == MODE_EXECUTE) {
        if(pte & PF_NX) return UVM_PROTECTION;
    }
    if((mode & MODE_ACCESS_MASK) == MODE_WRITE) {
        pte = pte | PF_DIRTY;
        if(memf) CLM;
        pt[ent] = pte;
        if(memf) STM;
    }
    return bits_remove(pte, PAGEMASK);
}

char *uvm_walk_pte(pgdir *pl3, char *va, char mode) {
    char *lvl;
    lvl = uvm_resolve(pl3, (va >> (3 * PAGEBITS)), mode);
    if(lvl < UVM_NUM_ERROR) return lvl;
    lvl = uvm_resolve(lvl, (va >> (2 * PAGEBITS)), mode);
    if(lvl < UVM_NUM_ERROR) return lvl;
    return lvl | ((va >> PAGEBITS) & PAGEMASK);
}

char uvm_walk(pgdir *pl3, char *va, char mode) {
    char *lvl = uvm_walk_pte(pl3, va, mode);
    if(lvl < UVM_NUM_ERROR) return lvl;
    lvl = uvm_resolve(bits_remove(lvl, PAGEMASK), (va >> PAGEBITS), mode);
    if(lvl < UVM_NUM_ERROR) return lvl;
    return lvl | (va & PAGEMASK);
}

void uvm_map(pgdir *pl3, char *va, char *pa, char flags) {
    char memf = insn_cpuget(REG_MF);
    char *pte;
    va = bits_remove(va, PAGEMASK);
    pa = bits_remove(pa, PAGEMASK);
    pte = uvm_walk_pte(pl3, va, MODE_MAP | MODE_POPULATE);
    if(memf) CLM;
    *pte = bits_remove(pa, PAGEMASK) | flags | PF_PRESENT;
    if(memf) STM;
}

void uvm_unmap(pgdir *pl3, char *va) {
    char memf = insn_cpuget(REG_MF);
    char *pte;
    va = bits_remove(va, PAGEMASK);
    pte = uvm_walk_pte(pl3, va, MODE_MAP);
    if(pte < UVM_NUM_ERROR) return;
    if(memf) CLM;
    *pte = 0;
    if(memf) STM;
}

void uvm_map_range(pgdir *pl3, char *va, char *pa, char size, char flags) {
    va = bits_remove(va, PAGEMASK);
    pa = bits_remove(pa, PAGEMASK);
    while(size > 0) {
        MOV [ES:-1], va;
        MOV [ES:-2], pa;
        MOV [ES:-3], size;
        uvm_map(pl3, va, pa, flags);
        pa += PAGESIZE;
        va += PAGESIZE;
        size -= PAGESIZE;
    }
}

void uvm_unmap_range(pgdir *pl3, char *va, char size) {
    va = bits_remove(va, PAGEMASK);
    while(size > 0) {
        MOV [ES:-1], va;
        MOV [ES:-2], size;
        uvm_unmap(pl3, va);
        va += PAGESIZE;
        size -= PAGESIZE;
    }
}

uvm_ptbl:
    // This entry is the only and default entry.
    // The runlevel has to be >0 to catch GPFs for privileged instructions (see cpu.txt).
    // Don't set any permission bits--those are handled in kspace above.
    db (MF_MAX_RUNLEVEL << MF_RUNLEVEL_SHIFT) | MF_OVERRIDE, 0;
    alloc 1024  // 128k memory model number of internal pages--8 pages. Some of these can be reclaimed.

uvm_scratch_write:
    // Rejected writes go here.
    db 0;

//pgdir *uvm_cur_pl3 = 0;
char *uvm_cur_pl3 = 0;
    
void uvm_memrd(char *stack, char mode, char intr, char ladd) {
    char *va = insn_cpuget(REG_MEMADDR);
    char *pa;
    MOV [ES:-3], va;
    MOV [ES:-4], ladd;
    if(va < 0) return;  // IO access (FIXME: always enabled)
    if(uvm_cur_pl3 == 0) {
        panic("no pgdir in uvm_read");
    }
    pa = uvm_walk(uvm_cur_pl3, va, MODE_READ);
    MOV port1, pa;
    if(pa < UVM_NUM_ERROR) {
        insn_cpuset(REG_LADD, 0);
        if(pa == UVM_PROTECTION) {
            MOV [ES:-1], -1;
            interrupt_trigger(stack, mode, INT_PPREAD, va);
        } else if(pa == UVM_NEXIST) {
            MOV [ES:-1], -2;
            interrupt_trigger(stack, mode, INT_IO, va);
        } else {
            MOV [ES:-1], -3;
            panic("uvm_walk failed with unknown error %d\n", pa);
        }
    } else {
        CLM;
        insn_cpuset(REG_LADD, *pa);
        MOV [ES:-1], *pa;
        STM;
    }
}

void uvm_memwr(char *stack, char mode, char intr, char ladd) {
    char *va = insn_cpuget(REG_MEMADDR);
    char *value = insn_cpuget(REG_LADD);
    char *pa;
    if(va < 0) return;  // IO access (FIXME: always enabled)
    if(uvm_cur_pl3 == 0) {
        panic("no pgdir in uvm_write");
    }
    pa = uvm_walk(uvm_cur_pl3, va, MODE_WRITE);
    if(pa < UVM_NUM_ERROR) {
        insn_cpuset(REG_MEMADDR, uvm_scratch_write);
        if(pa == UVM_PROTECTION) {
            interrupt_trigger(stack, mode, INT_PPWRITE, va);
        } else if(pa == UVM_NEXIST) {
            interrupt_trigger(stack, mode, INT_IO, va);
        } else {
            panic("uvm_walk failed with unknown error %d\n", pa);
        }
    } else {
        insn_cpuset(REG_MEMADDR, pa);
    }
}

void uvm_init(char pasize) {
    // None of these settings take effect until both MF/EF are enabled.
    // NB: interrupt_init does STEF.
    debug_printkln("uvm_init\n");
    insn_cpuset(REG_PTBL, uvm_ptbl);
    insn_cpuset(REG_PTBE, 0);
    interrupt_install(INT_MEMRD, uvm_memrd);
    interrupt_install(INT_MEMWR, uvm_memwr);
}

#endif
